CMD na pasta de instalação do Virtual Box -> VBoxManage modifyvm "Virtual Machine Name" --nested-hw-virt on //Isto faz com que a opção "Habilitar VT-x/AMD-V Aninhado" habilite
No PowerShell como administrador -> bcdedit /set hypervisorlaunchtype off //Isto faz com que o Hyperviser (Hyper-V) desabilite para as VMs funcionarem, mas o WSL para de funcionar
Encurtando o caminho no Gitbash: PS1="${debian_chroot:+($debian_chroot)}\[\033[01;34m\]\W \[\033[31m\]\$\[\033[00m\] "

Aula 01: Cluster com Docker Swarm -------

    O que é Docker Swarm?

        Vamos entender porque utilizar o docker da forma que estavamos utilizando não é o suficiente.

        O que podemos fazer com o Docker? Subir vários containers, com diversos serviços. Subimos 3, 6, 9 e assim por diante...O que acontece quando temos um número
        imenso de containers? 30, 60, 90 etc?

        Vamos lembrar que cada container é um processo a mais na máquina (Docker host).

        Então, em cada container temos um serviço e externamente, podemos ter diversos processos o que pode fazer com que a nossa máquina/servidor não aguente pois
        todos estarão consumindo muitos recursos, sejam eles, memória, processamento etc. Não sabemos exatamente o que está sendo executado em cada um destes containers.

        O que pode ocorrer é que um dos container falhe, talvez outros containers dependam do serviço que estava rodando neste container que caiu. Isto faz com que os
        outros container parem de funcionar mesmo que de maneira indireta. Isto demandará intervensão manual para que o container volte a funcionar da forma que antes
        estava funcionando. É neste momento que o Docker Swarm entra para resolver as coisas de uma forma "elegante".

        Como assim? Vamos imaginar aqui que temos agora uma rede de computadores, e o que podemos fazer com uma rede de computadores? Posso criar um cluster, um conjunto
        de máquinas agora para dividir determinado processamento, então, por exemplo, no primeiro caso qual é o problema? Que toda uma única máquina ali tem que carregar
        todos aqueles outros containers de uma vez só e isso poderia acabar sobrecarregando ela.

        Agora tendo um cluster, o que podemos fazer? Podemos instalar o Docker em todas essas máquinas de um mesmo cluster, que vamos ver como vai ser criado, e dividir
        esses containers, toda essa carga entre essas máquinas dentro de um cluster, e agora não vamos sobrecarregar mais uma máquina em específico, mas vamos dividir 
        toda essa carga entre elas.

        Isso também significa que vamos conseguir resolver aquele problema, também queremos que caso um container pare de funcionar ele volte ao normal da forma que 
        deveria. Como resolveremos isso? Precisamos ter alguém que faça o papel de orquestrar, de fazer todo esse controle do que vai ser colocado e onde, como ele vai
        fazer os serviços voltarem a funcionar no caso de pararem.

        E qual é o papel do orquestrador da forma que conhecemos? É reger uma orquestra, que nesse caso vão ser os próprios containers, e quem vai ser o orquestrador?
        Será o Docker Swarm, ele vai fazer o papel de definir em qual máquina vai ser melhor rodado determinado container, se um container está pronto para ser reiniciado
        em caso de falha ou não, e assim por diante.

        O Docker Swarm vai agir como um orquestrador num Cluster de máquinas. Através de um dispatcher ele vai definir qual máquina é a mais apropriada para rodar 
        determinado container.

        Ele faz de maneira autônoma a escolha da máquina para rodar determinado container. Isso já é uma coisa bem interessante porque já conseguimos evitar aqueles
        problemas todos de sobrecarga que podem acontecer.

        E como dito antes, pode ser que algum container também por algum motivo falhe, e o que o Docker Swarm consegue fazer? Ele consegue através de políticas de restart
        definir que esse contêiner vai voltar a funcionar de maneira autônoma, diretamente sem nenhuma intervenção, então o que estava ali com problema, vai parar de ter
        problema porque um novo serviço vai ser instanciado ali para nós.

        Vamos entender como vamos começar a criar o primeiro cluster, começar a criar um sistema com várias máquinas.


    Usando a Docker Machine:

        Já vimos que o Docker Swarm serve para que tenhamos diversas máquinas para dividir o nosso processamento. O legal é que com o Docker Machine não precisaremos de diversas máquinas para
        fazer isso, trabalharemos com apenas uma máquina física e as demais serão virtuais.

        Ela não é ferramenta crucial para que utilizemos o Docker Swarm, mas nos ajudará muito neste curso para fins didáticos. As máquinas virtuais que o Docker Machine criará formará o nosso
        cluster.

        A questão agora é...Como iremos criar estas máquinas virtuais? Como vamos utilizar o Docker Machine? Instalaremos o Docker Machine de acordo com este link:

            https://docs.docker.com/machine/install-machine/
        
        Com o comando '$ docker-machine' vemos uma sequência de comandos e instruções da aplicação...Vários comandos são bem parecidos com os do próprio Docker...
        Por exemplo:

            $ docker-machine ls - Irá nos listar todas as máquinas virtuais que criamos até o momento com o 'docker-machine'...

            $ docker-machine ls
            NAME   ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER      ERRORS

        Agora se quisermos criar uma Docker Machine, uma máquina virtual pronta para rodar o Docker precisaremos rodar o comando:

            $ docker-machine create ...

            precisamos também informar o driver que o docker-machine utilizará para criar esta máquina virtual, então por exemplo, existem diversos drivers, mas o que vamos utilizar aqui neste
            curso, por ser o mais famoso e o mais tranquilo de se instalar, será o VirtualBox...

            Então além de instalarmos o Docker Machine , nós também vamos precisar instalar o VirtualBox (Já realizado em cursos anteriores), e como informaremos isso? Acrescentando um '-d'ao 
            comando junto com 'virtualbox', por fim, colocamos o nome que queremos dar a nossa máquina virtual, neste curso no caso estou usando o mesmo que o instrutor 'vm1', então damos 'Enter'
            e agora ele começará a criar esta máquina...

            $ docker-machine create -d virtualbox vm1
            
            A saída será parecida com esta:
            
                Running pre-create checks...
                Creating machine...
                (vm1) Copying C:\Users\nesso\.docker\machine\cache\boot2docker.iso to C:\Users\nesso\.docker\machine\machines\vm1\boot2docker.iso...
                (vm1) Creating VirtualBox VM...
                (vm1) Creating SSH key...
                (vm1) Starting the VM...
                (vm1) Check network to re-create if needed...
                (vm1) Windows might ask for the permission to configure a dhcp server. Sometimes, such confirmation window is minimized in the taskbar.
                (vm1) Waiting for an IP...
                Waiting for machine to be running, this may take a few minutes...
                Detecting operating system of created instance...
                Waiting for SSH to be available...
                Detecting the provisioner...
                Provisioning with boot2docker...
                Copying certs to the local machine directory...
                Copying certs to the remote machine...
                Setting Docker configuration on the remote daemon...
                
                This machine has been allocated an IP address, but Docker Machine could not
                reach it successfully.
                
                SSH for the machine should still work, but connecting to exposed ports, such as
                the Docker daemon port (usually <ip>:2376), may not work properly.
                
                You may need to add the route manually, or use another related workaround.
                
                This could be due to a VPN, proxy, or host file configuration issue.
                
                You also might want to clear any VirtualBox host only interfaces you are not using.
                Checking connection to Docker...
                Docker is up and running!
                To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: C:\Users\nesso\bin\docker-machine.exe env vm1

        Este processo demora um tempo, onde baixa todos os arquivos necessários...
        Se tudo der certo, ao final, somos informados que a máquina está rodando e que agora podemos nos conectar à ela...Podemos utilizar agora aquele comando para listar todas as máquinas
        virtuais em execução...

            $ docker-machine ls

            Agora nos é apresentada a seguinte, ou semelhante, saída:

            NAME   ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER      ERRORS
            vm1    -        virtualbox   Running   tcp://192.168.99.106:2376           v19.03.12

            Temos as informações do nome da máquina virtual, o driver que ela está utilizando, a URL e a versão do docker que está instalada na máquina...
            Podemos nos conectar à esta máquina realizando o seguinte comando:

                $ docker-machine ssh vm1

                A seguinte saída nos é apresentada:

                    ( '>')
                   /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
                  (/-_--_-\)           www.tinycorelinux.net
        
        
            Se caso tivessemos criado a máquina mas ela não estivesse rodando, poderíamos iniciá-la com o comando:

                $ docker-machine start "nome da vm"

            Se utilizarmos este comando agora com a máquina que acabamos de criar, seremos informados que a máquina já está rodando...

                Starting "vm1"...
                Machine "vm1" is already running.

            Agora que já aprendemos o básico com docker-machine, vamos avançar com o curso...

        A partir de agora, iremos criando as nossas máquinas virtuais conforme formos precisando e, por fim, construir o nosso cluster...Só pra frizar mais uma vez, o que é um Cluster: Nada mais
        que um conjunto de máquinas, físicas ou não, que dividem o processamento de determinadas aplicações, criaremos o nosso Swarm a partir destas máquinas virtuais leves.
        
        Então, só para dar aquela finalizada aqui, se dermos um 'docker -v' dentro da máquina virtual que criamos, ela vai listar a mesma versão que foi listada pela Docker Machine com o comando
        '$ docker-machine ls', confirmando aqui que temos uma máquina virtual já com o Docker instalado. A ideia agora é ver como vamos criar o nosso primeiro cluster.

    
    Para saber mais: Cloud e Docker Machine:

        Durante o curso, utilizaremos a Docker Machine apenas para criarmos nosso ambiente com diversas máquinas isoladas e iniciarmos nosso cluster.
        Porém, a Docker Machine também é muito utilizada com provedores de serviço em nuvem, como a AWS! Podemos definir nossas credenciais, e, utilizando o driver amazonec2, temos a 
        possibilidade de criar diversas máquinas nos servidores da Amazon!        
        Caso tenha interesse, mais informações podem ser obtidas na documentação oficial.

    
    Criando o Cluster:

        A partir da primeira máquina virtual que criamos, iremos iniciar o nosso cluster...
        A partir de agora, sempre que formos nos referir ao nosso cluster, vamos chamar ele de Swarm, porque além de ser o nome da ferramenta, ele remete ao coletivo de animais em inglês,
        enxame, bando, alcateia, e tem toda essa ideia de trabalho coletivo, de divisão de trabalho assim como queremos fazer com o Docker Swarm.

        O que precisamos agora para criar o nosso Cluster? Precisamos acessar a nossa máquina virtual recém-criada, e já vimos que para fazer isso, executamos o comando:
            
            $ docker-machine ssh vm1

        Agora precisamos ececutar um comando bem simples que é:

            $ docker swarm init (Já dentro da máquina, nunca esquecer disso)

        Nos é apresentado o seguinte erro:

            docker@vm1:~$ docker swarm init
            Error response from daemon: could not choose an IP address to advertise since this system has multiple addresses on different interfaces (10.0.2.15 on eth0 and 192.168.99.106 on eth1) 
            - specify one with --advertise-addr

        Se lermos o erro com calma, veremos que ele não conseguiu anunciar o IP que vai ser utilizado por essa máquina dentro do Swarm, ele tem tanto a possibilidade de usar o IP da máquina
        virtual quanto o IP da minha máquina física, que são os dois drivers que ele está se referenciando o eth0 (10.0.2.15) e o eth1 (192.168.99.106). Ele está dizendo que eu tenho que
        especificar o IP que eu quero através da '--advertise-addr'.

        Mesmo que não tivessemos este erro, é sempre boa prática usarmos essas flags da '--advertise-addr', porque eu preciso fixar um IP para a minha máquina criadora do meu Swarm, para que 
        todas as máquinas que foram entrar daqui para frente, elas consigam manter uma comunicação fixa e estável com quem criou o Swarm.

        Então agora vamos tentar iniciar o Swarm da forma correta com o comando:

            $ docker swarm init --advertise-addr "IP da máquina "Física""

        "Física", porque na verdade é o IP da máquina virtual que estamos utilizando...
        Agora temos a mensagem de que o Swarm foi inicializado:

            Swarm initialized: current node (uf6tlac8zhq0m8an77xqgkjhx) is now a manager.

            To add a worker to this swarm, run the following command:
            
                docker swarm join --token SWMTKN-1-3w716knv7dxqhaax04xpxjau0zh2tmkntybpr3g10sl403v3f3-bblekngv16esceoqytg2ugyef 192.168.99.106:2377
            
            To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

        Como temos a certeza de que esta máquina agora está em um Swarm?
        Utilizamos o comando:

            $ docker info

            Na parte 'Swarm:' ela deve estar 'active'

        Então todo o trabalho que teríamos se quiséssemos implementar essa ideia de divisão de carga e de políticas de restart na mão, já foi feita com um simples comando, e agora ele está
        falando que além do Swarm ter isdo incializado, o meu nó corrente tem esse ID (uf6tlac8zhq0m8an77xqgkjhx) e é um manager, e quem é esse sinal corrente? É exatamente essa máquina que
        criou o Swarm, porque a máquina que criou o Swarm, ela a partir desse momento vai ser considerada manager e líder desse Swarm.

        E agora o que ela precisa ter? Workers ou "Trabalhadores" para realizar as tarefas árduas que vão ser impostas daqui pra frente. A partir de agora precisamos ententer o que são esses
        tais de workers, como adicionar eles, qual o papel deles dentro do nosso Swarm e será que precisamos adicionar outros managers?

    
    Nesta aula, aprendemos:

        - O Docker Swarm é um orquestrador
        - O Docker Swarm é capaz de alocar e reiniciar containers de maneira automática
        - Como criar máquinas já provisionadas para utilizar o Docker com a Docker Machine utilizando comando docker-machine create
        - Um cluster é um conjunto de máquinas dividindo poder computacional
        - Como criar um cluster utilizando o Docker Swarm utilizando o comando docker swarm init

        
    
    Questões aula 01:

        01: O Docker Swarm apresenta uma série de vantagens em relação ao Docker usado de maneira tradicional. Marque as alternativas que contém essas vantagens:

            Selecione 2 alternativas

            R1: O Docker Swarm divide os containers entre múltiplas máquinas de um mesmo cluster de maneira automática.
                Alternativa correta! Através do dispatcher o Docker Swarm define a melhor máquina para executar algum container

            R2: O Docker Swarm consegue resetar containers automaticamente em caso de falhas.
                Alternativa correta! O Docker Swarm tem capacidade de reiniciar containers a fim de manter a aplicação funcionando.

        
        02: Vimos na última aula que a Docker Machine, por mais que não esteja relacionada diretamente ao Docker Swarm, pode nos ajudar bastante. Qual das alternativas abaixo contém uma 
            funcionalidade da Docker Machine?

            Selecione uma alternativa

            R: Ao utilizar a Docker Machine, podemos criar máquinas virtuais prontas para executar Docker.
               Alternativa correta! A Docker Machine cria máquinas virtuais bem leves já provisionadas com o Docker.

        
        03: Queremos criar nosso primeiro cluster para dividir os containers em diversas máquinas e não sobrecarregar uma única máquina. Qual dos comandos abaixo devemos utilizar para criar o 
            cluster e darmos o primeiro passo para atingir nosso objetivo?

            Selecione uma alternativa

            R: docker swarm init
               Alternativa correta! Além disso, a boa prática também seria utilizar a flag --advertise-addr.


Aula 02: Responsabilidade dos nós workers -------

    Criando o primeiro worker:

        Temos até agora o nosso Swarm, temos uma máquina fazendo parte desse Swarm, é um nó, que é o nosso manager, o nosso líder até então, mas precisamos agora de alguém para realizar os
        trabalhos, que carrega os contêiners e faça toda a aparte mais pesada de processamento.

        Ou seja, temos o manager que é a máquina principal, mas precisamos das outras que ajudarão a manager a gerenciar, para isto que serve o Swarm.

        Vamos ter o nosso líder, e ele terá seus "subordinados", e esses subordinados vão fazer exatamente o papel de carregar nossos containers enquanto o principal cordena tudo que vai
        acontecer, vimos nas primeiras aulas que o computador principal faz o papel de orquestrador enquanto teremos os caras que vão ser responsáveis pelo carregamento das informações,
        processamento de containers e tudo mais.

        A pergunta que fica agora se formos observar é, como vamos criar esses dois workers? Se formos pensar um pouco mais abstrato, estamos colocando duas novas máquinas, dois novos nós dentro
        desse Swarm, precisamos criar duas novas máquina virtuais, e para criarmos novas máquinas, já sabemos que é só utilizar aquele clássico comando 'docker-machine create -d virtualbox' e 
        aqui passamos o nome da VM. No caso segui a sequencia como o instrutor e coloquei 'vm2' e 'vm3'.

        A questão agora é como estes caras vão ser inseridos no Swarm, se buscarmos no terminal, quando criamos o nosso Swarm ele já deu esse comando. Rode o seguinte comando para adicionar mais
        nós ao Swarm (docker swarm join --token SWMTKN-1-3w716knv7dxqhaax04xpxjau0zh2tmkntybpr3g10sl403v3f3-bblekngv16esceoqytg2ugyef 192.168.99.106:2377).

           $ docker swarm join --token SWMTKN-1-3w716knv7dxqhaax04xpxjau0zh2tmkntybpr3g10sl403v3f3-bblekngv16esceoqytg2ugyef 192.168.99.106:2377

        Como ele sabe que o Swarm que tem que se conectar é o que criamos? Por causa do IP! 192.168.99.106 na porta 2377. Esta é a nossa manager, que é a porta que o Docker Swarm necessita para
        fazer a comunicação entre os nós que vão fazer parte de um Swarm.

        Após terminar de criar as máquinas teremos que acessá-las através do comando 'docker-machine ssh 'nomeDaMaquina''. Depois de estar dentro da máquina, basta usar o comando informado 
        na criação do Swarm, aquele que citamos acima, e colar na linha de comando, dentro da máquina virtual acessada. Veremos a seguinte saída:

            docker@vm2:~$ docker swarm join --token SWMTKN-1-3w716knv7dxqhaax04xpxjau0zh2tmkntybpr3g10sl403v3f3-bblekngv16esceoqytg2ugyef 192.168.99.106:2377
            This node joined a swarm as a worker.

        O comando é grande, mas não precisamos decorá-lo. Tecnicamente não teriamos mais aquele comando disponível no terminal, após executar um clear, exit etc...
        O que precisamos fazer se quisermos recuperar aquele comando para que possamos adicionar mais workers? É simples...Basta que executemos o seguinte comando dentro da máquina manager:

            docker@vm1:~$ docker swarm join-token worker
            To add a worker to this swarm, run the following command:
            
                docker swarm join --token SWMTKN-1-3w716knv7dxqhaax04xpxjau0zh2tmkntybpr3g10sl403v3f3-bblekngv16esceoqytg2ugyef 192.168.99.106:2377

        Ali ele nos retornou o comando necessário para adicionar novos workers...

        Então agora precisamos entender, depois que nós já adicionamos esse primeiro líder e agora temos nosso workers, como esses caras vão conseguir se comunicar, como conseguimos listar o
        que é que tem dentro do nosso Swarm. Precisamos arrumar um pouco a casa para entender que no fim das contas esse líder vai fazer o papel de orquestrar, de definir tudo que vai acontecer
        aqui dentro, enquanto nossos workers vão fazer o trabalho de carregar os nossos containers.

    Listando e removendo nós:

        Até o momento aprendemos a criar o nosso Swarm com um manager dentro, conseguimos já adicionar workers também a esses Swarms, mas ainda precisamos saber como listar quais são os nós e
        quantos são eles dentro do Swarm e como conseguimos remover também os nós a partir do momento que eles estão fazendo parte de um Swarm.

        Apesar de nós sabermos no momento que adicionamos um manager e dois workers, totalizando três nós, precisamos de uma forma sucinta para consultar isso, por isso temos o comando:

            $ docker node ls

            Ele lista para nós o ID, o hostname, o status, se está disponível, se é lider ou não do swarm e a versão da "engine" instalada, ou seja, a versão do docker que eles estão rodando.

            ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
            uf6tlac8zhq0m8an77xqgkjhx *   vm1                 Ready               Active              Leader              19.03.12
            j9wyh6w89cpk2eh1hx749n7kv     vm2                 Ready               Active                                  19.03.12
            tycqgy7kndfjaznweg11xprsg     vm3                 Ready               Active                                  19.03.12

        Então por exemplo, ele listou os IDs dos nossos nós e no nó que rodou o comando ele colocou um asterisco. Vemos que é nossa vm1 que rodou o comando, o status é 'ready', que significa
        que está rodando sem nenhum problema.

        O que os outros nós diferem do primeiro? Que eles não tem a informação em 'MANAGER STATUS', se eles não tem nada de manager status dentro deles, significa para o Swarm que eles são
        workers, então esses dois caras aqui por não terem informações nessa coluna, significa que são apenas workers do Swarm.

        Mais uma outra pergunta que ficaria: Será que eu posso executar esse 'docker node ls' em qualquer nós do meu Swarm? Se conectarmos ao nosso segundo nó, 'vm2', e tentarmos executar o 
        mesmo comando, teremos a seguinte saída: 
        
            "Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the 
                current node to a manager."

            Ele detalha tudo aqui perfeitamente para nós, ele fala "Esse nó não é um manager do Swarm. Workers nnão podem ser usados para visualizar ou alterar o estado do seu cluster. Por favor
            rode este comando em um nó manager ou promova o nó, no qual você se encontra, para manager."

        Isto significa que se tentassemos na nossa vm3 ou em outros nós que não sejam manager teremos a mesma mensagem de erro.

        Uma coisa que falta ainda, agora que conseguimos adicionar e listar os nossos nós, precisamos fazer esse papel de remoção. Podemos tentar remover um nó primeiro executando o comando:

            $ docker node ls

            Então verificamos qual o é o ID do nó que queremos remover, e então executamos o comando:

            $ docker node rm "ID do nó"

        Receberemos a seguinte saída:
                    
            "Error response from daemon: rpc error: code = FailedPrecondition desc = node tycqgy7kndfjaznweg11xprsg is not down and can't be removed"

            Aqui diz que:

            "Este nó não está down e não pode ser removido"

        Então, para que possamos deixar nossa vm3 como down, temos que nos conectar à ela e executar o seguinte comando:

            $ docker swarm leave

            Receberemos a seguinte saída:

                "Node left the swarm."

        Apesar de que um nó não tem autonomia para alterar o Swarm, ele é responsável pela mudança de seu status, ação pré-requerida para que seja removido do Swarm pelo nó líder...
        Sabendo disso agora voltamos ao nosso nó principal e executamos novamente o comando: $ docker node rm "ID do nó"

        Primeiro vamos executar o comando: "$ docker node ls" para verificar se realmente o status do nós 3 foi modificado

            ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
            uf6tlac8zhq0m8an77xqgkjhx *   vm1                 Ready               Active              Leader              19.03.12
            j9wyh6w89cpk2eh1hx749n7kv     vm2                 Ready               Active                                  19.03.12
            tycqgy7kndfjaznweg11xprsg     vm3                 Down                Active                                  19.03.12

            Podemos verificar que agora o STATUS do nó 3 está Down...Vamos ao comando de remover o nó agora...

            $ docker node rm tycqgy7kndfjaznweg11xprsg
            
            Agora se executarmos mais uma vez o comando: "$ docker node ls" veremos a seguinte saída...

            ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
            uf6tlac8zhq0m8an77xqgkjhx *   vm1                 Ready               Active              Leader              19.03.12
            j9wyh6w89cpk2eh1hx749n7kv     vm2                 Ready               Active                                  19.03.12

            Confirmamos que o nó 3 foi removido...Quem realmente executou a exclusão do nó foi o nosso 'Leader'
        
        Agora vamos readicionar o nó 3 ao Swarm...
        Realizamos o comando "docker swarm join-token worker" para pegar o comando com a chave necessária para adição...

            Nossa saída...

            To add a worker to this swarm, run the following command:

            docker swarm join --token SWMTKN-1-3w716knv7dxqhaax04xpxjau0zh2tmkntybpr3g10sl403v3f3-bblekngv16esceoqytg2ugyef 192.168.99.106:2377

        Utilizamos este comando no nó 3 novamente...

            "This node joined a swarm as a worker."

            Executamos agora o comando "docker node ls" novamente no nó líder...

            E...O nó 3 está presente mais uma vez...

            ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
            uf6tlac8zhq0m8an77xqgkjhx *   vm1                 Ready               Active              Leader              19.03.12
            j9wyh6w89cpk2eh1hx749n7kv     vm2                 Ready               Active                                  19.03.12
            u1hquwd0mnvktpn9d7wcqvogu     vm3                 Ready               Active                                  19.03.12

        Até o momento então aprendemos como:

            Listar, adicionar, criar um cluster e outras operações básicas dentro do Swarm.

    
    Subindo um serviço:

        Chegou o momento em que vamos criar o nosso primeiro container dentro do nosso Swarm. Onde iremos subir este container? O ideal é que façamos isso em um dos nosso workers, ou seja, na
        vm1 ou vm3, pois é o trabalho deles executar os container, fazer toda a carga pesada, enquanto, a princípio, o trabalho de orquestração e de leitura de escritas vai ser realizada pelo
        nosso manager.

        Se queremos subir um container, já aprendemos isso no curso de docker, podemos ir no terminal do nó que desejamos, por exemplo, vm2 que é um dos nossos workers e executar aquele comando
        "docker container run" passando a porta em que queremos fazer o binding (Mapear atributos em requisições HTTP para objetos ou mapas), no caso a porta "8080" da nossa vm2 com a porta "3000"
        do container que criamos, colocamos para isso o "-d" para não travar o nosso terminal, e por fim colocamos "aluracursos/barbearia" que é o nome da imagem que iremos utilizar...

            $ docker container run -p 8080:3000 -d aluracursos/barbearia

            O download da imagem irá iniciar...

        Após o download podemos executar um "docker container ls" para verificar que o nosso container está em execução, e podemos conferir que ele está fazendo binding da porta 8080 com a 3000.

            CONTAINER ID        IMAGE                   COMMAND                  CREATED             STATUS              PORTS                    NAMES
            e851f40e17bf        aluracursos/barbearia   "/bin/sh -c 'node se…"   2 minutes ago       Up 2 minutes        0.0.0.0:8080->3000/tcp   laughing_blackbur

        Como é que acessamos essa aplicação agora? A ideia inicial é irmos até o browser e coloquemos "localhost:8080", mas fazendo isso teremos uma mensagem de erro...Isto porque o nosso
        container está executando em nossa vm2 e não na nossa máquina local propriamente dito...Esta vm2 tem seu próprio IP e suas próprias configurações básicas. A nossa máquina física nesse
        curso, está sendo usada só para emular as máquinas que estamos criando para colocar no nosso Swarm.

        Para acessar a aplicação que acabamos de subir com o container, precisamos descobrir o endereço IP da vm2. Ela por ser um nó do nosso Swarm, podemos conseguir informações detalhadas dela
        pelo Leader. Assim como o Docker, usando ele Stand Alone que fizemos no curso anteior, conseguíamos entender e inspecionar diversas informações sobre um container, sobre uma imagem, sobre
        um volume...Faz todo o sentido que agora consigamos também ver informações bem mais detalhadas sobre um determinado nó.

        Para inspecionar um nó, usaremos o seguinte comando dentro do nosso manager que é a nossa vm1:

            $ docker node inspect vm2

            Teremos a seguinte saída:

            [
                {
                    "ID": "j9wyh6w89cpk2eh1hx749n7kv",
                    "Version": {
                        "Index": 35
                    },
                    "CreatedAt": "2021-03-25T00:56:22.33036158Z",
                    "UpdatedAt": "2021-03-31T18:11:50.593637781Z",
                    "Spec": {
                        "Labels": {},
                        "Role": "worker",
                        "Availability": "active"
                    },
                    "Description": {
                        "Hostname": "vm2",
                        "Platform": {
                            "Architecture": "x86_64",
                            "OS": "linux"
                        },
                        "Resources": {
                            "NanoCPUs": 1000000000,
                            "MemoryBytes": 1033252864
                        },
                        "Engine": {
                            "EngineVersion": "19.03.12",
                            "Labels": {
                                "provider": "virtualbox"
                            },
                            "Plugins": [
                                {
                                    "Type": "Log",
                                    "Name": "awslogs"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "fluentd"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "gcplogs"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "gelf"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "journald"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "json-file"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "local"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "logentries"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "splunk"
                                },
                                {
                                    "Type": "Log",
                                    "Name": "syslog"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "bridge"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "host"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "ipvlan"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "macvlan"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "null"
                                },
                                {
                                    "Type": "Network",
                                    "Name": "overlay"
                                },
                                {
                                    "Type": "Volume",
                                    "Name": "local"
                                }
                            ]
                        },
                        "TLSInfo": {
                            "TrustRoot": "-----BEGIN CERTIFICATE-----\nMIIBajCCARCgAwIBAgIUKEOhSAjR7FPZKcH+iPMMA2T27i8wCgYIKoZIzj0EAwIw\nEzERMA8GA1UEAxMIc3dhcm0tY2EwHhcNMjEwMzIwMTkyODAwWhcNNDEwMzE1MTky\nODAwWjATMREwDwYDVQQDEwhzd2FybS1jYTBZMBMGByqGSM49AgEGCCqGSM49AwEH\nA0IABHOI4VRH807sqAc2IA+v7Y5KmpSEn41cCC9eKxp5LqV682AWrLutBZBaoMFY\nvVdiOyPONXH9B8AzdJD6VE3ifdyjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMB\nAf8EBTADAQH/MB0GA1UdDgQWBBQaMA1e0wbub/ig0gbb7etkuyyWPTAKBggqhkjO\nPQQDAgNIADBFAiA316oy+8K/wQWS6d4r84liL9t8ulOKxw776XLMIEkMCQIhAI00\ndVqnQmslCaafl4GeQJ7t/huE2m6BKP5T+8Rs6Dg6\n-----END CERTIFICATE-----\n",
                            "CertIssuerSubject": "MBMxETAPBgNVBAMTCHN3YXJtLWNh",
                            "CertIssuerPublicKey": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEc4jhVEfzTuyoBzYgD6/tjkqalISfjVwIL14rGnkupXrzYBasu60FkFqgwVi9V2I7I841cf0HwDN0kPpUTeJ93A=="
                        }
                    },
                    "Status": {
                        "State": "ready",
                        "Addr": "192.168.99.107"
                    }
                }
            ]

            Muitas informações sobre o nosso nó foram exibidas, a informação que precisamos está no último bloco "Status", ele mostra o endereço IP, que é 192.168.99.107, se formos agora 
            novamente no browser e colocarmos o endereço 192.168.99.107:8080, veremos o site da "Barbearia Alura" ser exibido.

        Mas...Vamos pensar na estrutura do nosso Swarm...Será que a nossa vm1 e vm3 enxergam este container com a aplicação que está executando na vm2? Será que sabem de algum modo da existência
        dele? Porque a função do Swarm além de toda divisão do trabalho é manter nossa aplicação sempre disponível.

        A ideia é que todos os nós saibam o que está sendo feito para que eles consigam se comunicar de maneira correta, para não ter nenhum tipo de telefone sem fio dentro do Swarm.

        E para confirmarmos de uma maneira bem simples que os nossos nós sabem da existência desse container, é irmos em cada um deles, por exemplo, e executar um "docker container ls"...
        Verificamos que não foi exibido nada em nenhum dos outros nós (vm1, vm3), enquanto na vm2 está sendo exibidos os dados deste container...

        Por quê isto está acontecendo? Porque com o comando "docker container run" na vm2 fizemos com que o container e sua aplicação rodassem em escopo local, e não no escopo do Swarm como
        queremos. Como então podemos criar um container e rodar ele no escopo do Swarm? O primeiro passo será remover o container que criamos...

            $ docker container rm e8 --force

            removido...

        Agora queremos subir esse container de uma forma que todos saibam da existência dele, e como podemos fazer isso? Como queremos subir um container no Swarm de forma que todos vejam
        não trataremos mais como um simples container, mas sim como um serviço.

        Para criarmos um serviço dentro do nosso Swarm, temos um comando específico para isso, que é o "docker service create", após isso a sintaxe é bem parecida com a da criação de um container
        comum.

        Inserimos o nosso "-p 8080:3000", não precisamos mais do "-d", porque este novo comando por padrão não trava o terminal, e, por fim, a imagem que queremos utilizar, "aluracursos/barbearia"...
        Agora como resultado temos aquele mesmo erro que vimos aquela hora que tentamos alterar o Swarm através de um simples worker:
        "Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. 
            Please run this command on a manager node or promote the current node to a manager.", o motivo é o mesmo, como agora estamos tentando criar um serviço
        no escopo do Swarm e não um simples container local, não temos autonomia para realizar esta alteração à partir de um simples worker.

        Sendo assim para que possamos realizar com sucesso esta tarefa precisamos executar este comando dentro do nosso manager, a nossa vm1...

            docker@vm1:~$ docker service create -p 8080:3000 aluracursos/barbearia

        Temos a seguinte saída:

            lrtj4rn0a8nkpl6pysqg3wwfx
            overall progress: 1 out of 1 tasks
            1/1: running   [==================================================>]
            verify: Service converged
        
        Agora ele carregou tudo certinho e informou que o serviço convergiu.


    Tarefas e o Routing Mesh:

        Serviço convergido nada mais é do que serviço criado. Agora temos um container sendo executado no escopo do Swarm. Agora como listamos os serviços/containers? Podemos realizar o famoso
        comando do Docker:

            $ docker container ls

            Nossa saída...

            CONTAINER ID        IMAGE                          COMMAND                  CREATED             STATUS              PORTS               NAMES
            6c866926aa53        aluracursos/barbearia:latest   "/bin/sh -c 'node se…"   11 minutes ago      Up 11 minutes                           strange_hypatia.1.owo4fl9prahzfrejlr7rtvvph

        Vamos usar também o comando:

            $ docker service ls

            Nossa saída

            ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
            lrtj4rn0a8nk        strange_hypatia     replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp
            
        Temos o ID do container, o nome, etc...Vamos entender agora o que quer dizer replicado....
        Podemos ver que ele está fazendo binding na porta 8080 com a 3000, mas nós não sabemos uma coisa ainda, em qual máquina ele está sendo executado? Será que á a vm1, vm2 ou vm3? Porque
        temos um asterisco na frente da porta, não sabemos de qual vm é...

        Se voltarmos nas informações do console, veremos que ele também criou uma tarefa...

            "overall progress: 1 out of 1 tasks"

        ...e o que é essa tal de tarefa? O que isso quer dizer? Uma tarefa nada mais é do que a instância de um serviço que foi criado, então vamos parar para pensar, temos um serviço que criamos,
        uma ideia a ser realizada e criamos logo depois de uma tarefa que é a instância desse serviço na prática, se parar para pensar, é como se o serviço fosse meioo que a receita e a task que
        é só a execução desa receita propriamente dita.

        Se quisermos ver aonde esta tarefa essa tarefa está sendo executada, podemos vir aqui e dar um "docker service ps" e por fim, botar o ID do serviço que queremos ver o detalhe, quais tasks
        esse serviço tem.

        Vamos usar aqui "lr", que é o inicio da ID, dando um 'Enter', ele mostra diversas informações, como por exemplo o ID dessa tarega e o nome. Repare que é o mesmo nome do nosso serviço
        seguido de '.1' a imagem que ele está executando. Ele está utilizando a vm1, o estado desejado é que ele esteja rodando e ele realmente está e não vemos nenhum erro...

            docker@vm1:~$ docker service ps lr
            ID                  NAME                    IMAGE                          NODE                DESIRED STATE       CURRENT STATE             ERROR                              PORTS
            owo4fl9prahz        strange_hypatia.1       aluracursos/barbearia:latest   vm1                 Running             Running 26 minutes ago

        Então perfeito, sabemos que agora o nosso serviço, a nossa tarega está sendo executada na nossa vm1, ou seja, tem um container sendo executado na nossa vm1, só que agora ele está no
        escopo do Swarm, e o que isso muda? Porque agora é legal ter isso no escopo do Swarm? Se voltarmos na nossa aplicação e dermos F5 (192.168.99.106:8080), ainda estará funcionando, mas 
        agora o que conseguimos fazer?

        Para verificar as principais diferenças, vamos executar um "docker node inspect ...", para ver o IP da nossa vm2 e vm3...Verificamos que o IP da vm2 é '192.168.99.107'
        e da vm3 é '192.168.99.108'. Se colocarmos estes IPs no browser na porta 8080 será que funciona? Se fizermos os testes veremos que sim.

        Então, por mais que o serviço esteja sendo executado, a princípio o container esteja sendo mantido pela vm1, qualquer outra máquina consegue acessar esse container agora.

        E como isso acontece? Isso acontece porque agora quando fazemos essa requisição para a porta 8080 para o nosso Swarm, não mais para um nó específico, através de um cara chamado
        Routing Mesh, ele consegue fazer esse redirecionamento, e ele vai procurar se tem algum aplicação sendo executada em alguma porta 8080 de algum nó dentro do nosso Swarm, ele vai ver,
        nesse caso, que temos um serviço na vm1, então qualquer outra máquina, qualquer outro nó que recebe uma requisição para a porta 8080, esta requisição vai ser redirecionada para o nó que
        tem o serviço de fato. Isto significa que a porta 8080 está sendo utilizada no escopo todo do Swarm.

        E se tentarmos subir outra aplicação na porta 8080? Teremos um erro...

            Error response from daemon: rpc error: code = InvalidArgument desc = port '8080' is already in use by service 'strange_hypatia' (lrtj4rn0a8nkpl6pysqg3wwfx) as an ingress port

        O erro mostra que já temos o serviço 'strange_hypatia', utilizando a porta desejada, e que não pode ser utilizada como uma porta de ingresso.

        Só para vermos um pouco mais do que o Docker Swarm é capaz de fazer, se viermos na nossa vm1 novamente, e executarmos o comando "docker container ls"...

            CONTAINER ID        IMAGE                          COMMAND                  CREATED             STATUS              PORTS               NAMES
            6c866926aa53        aluracursos/barbearia:latest   "/bin/sh -c 'node se…"   About an hour ago   Up About an hour                        strange_hypatia.1.owo4fl9prahzfrejlr7rtvvph

        ...veremos que no fim das contas temos realmente um container sendo executado aqui, e esse container foi criado pela tarefa que veio lá do serviço que criamos.

        Se executarmos o comando "docker container rm" e colocarmos o ID do container que está executando o nosso serviço, início 6c, acompanhado de '--force' ao final, removemos esse container,
        podemos verificar com "docker container ls", que o container foi removido. 
        
            Obs: Como estamos na máquina 'leader' notamos que já existe outro container mas com o ID diferente, ou seja, 
            o antigo container foi removido, isto aconteceu porque o Swarm criou o serviço no mesmo nó. Este teste funciona melhor em máquinas que são simplesmente workers, pois de fato o 
            container some. Veremos mais pra frente no curso como deixar o nó leader apenas como gerenciador dos outros nós, e não sendo mais um nó no qual o Swarm pode alocar serviços.

        Como executamos o "docker container ls" na máquina leader, e vimos que foi criado um novo container, podemos agora executar o comando "docker service ps" acrescentando o ínicio do ID do
        serviço 'lr' veremos que agora o serviço está rodando no nó vm2, o serviço foi realocado automaticamente pelo Swarm em outro nó.

        Agora o "docker container ls" na vm1 não mostra mais nada...Podemos fazer mais vezes este mesmo teste, indo na nossa vm2 e removendo o container para ver o processo se repetindo.
        Vimos que da forma que está configurado o serviço acabou caindo dentro da nossa manager...E estudaremos sobre isso mais a frente. Avançaremos nos estudos sobre a manager.


        Nesta aula, aprendemos:

        - Que nós workers são responsáveis por executar containers
        - Comandos de leitura e visualização de nós, como o docker node ls
        - Comandos que leem ou alteram o estado do swarm só podem ser executados em nós managers
        - O comando docker container run sobe containers em escopo local e o docker service create cria serviços em escopo do swarm
        - Tarefas são instâncias de serviços
        - Portas são compartilhadas entre nós do swarm e são acessíveis a partir de qualquer nó graças ao routing mesh
        
        
    Questões aula 02:

        01 - Agora queremos adicionar nós ao swarm. Qual das alternativas abaixo é realmente uma responsabilidade dos nós workers dentro do swarm?

            Selecione uma alternativa

            R: São responsáveis pela execução dos containers dentro do swarm.

            Alternativa correta! Como o nome diz, eles são os trabalhadores, responsáveis por rodar containers.

        
        02 - Para executarmos comandos de leitura, como por exemplo docker node ls, e/ou alteração no estado do swarm, temos uma condição. Qual das alternativas abaixo contém essa condição?

            Selecione uma alternativa

            R: Podem ser executados apenas em nós managers.

            Alternativa correta! Tais comandos ficam restritos apenas aos nós managers dentro do swarm.

        
        03 - Vimos na última aula uma diferença bem importante entre os comandos docker service create e docker container run. Qual é essa diferença?

            Selecione uma alternativa
            
            R: O comando docker service create no final cria um container em escopo do swarm e o docker container run em escopo local.

            Alternativa correta! Como vimos, essa é uma grande diferença entre os dois comandos.

        
        04 - Qual artifício do Docker Swarm permite que nós possamos acessar quaisquer serviços a partir do IP de qualquer nó dentro do swarm, apenas informando a porta?

            Selecione uma alternativa

            R: Routing Mesh.

            Alternativa correta! Graças ao Routing Mesh conseguimos acessar diferentes serviços a partir de qualquer IP pertencente ao swarm.


Aula 03: Gerenciando o cluster com managers -------

    O papel do manager:

        Nessa aula agora vamos ter um estudo um pouco mais teórico, mas ainda também prático sobre o papel efetivo dos managers dentro do nosso Swarm.

        Na aula passada estudamos sobre worker, como subimos agora um serviço para que consigamos subir um container no escopo do Swarm e essas coisas. Mas agora queremos saber qual é o papel
        do manager, será que ele só orquestra e administra o container? O que pode acontecer de diferente, o que pode ser causado de problemas para nós? Vamos imaginar um caso mais real...

        Vamos supor que sejam três máquinas reais, máquinas físicas, temos por exemplo máquina 1, máquina 2 e máquina 3, assim como estão dispostos nossos nós, elas estão em algum lugar do mundo.

        Aí de repente a nossa máquina 1 pega fogo, queima, sei-lá, é destruída, o que vai acontecer? A minha máquina 1 vai parar de funcionar, consequentemente eu perdi o meu manager, eu não tenho
        mais um manager no meu Swarm, o que isso pode acarretar de problema para mim? Muitas coisas.

        Ainda temos o nosso serviço e nossos nós funcionando da aula anterior, e se tentarmos simular esta situação onde o nó manager é perdido? Temos o comando "docker swarm leave", se tentarmos
        executar este comando, o próprio Docker nos enviará a mensagem:

        "Error response from daemon: You are attempting to leave the swarm on a node that is participating as a manager. Removing the last manager erases all current state of the swarm. 
                        Use `--force` to ignore this message."

        Esta mensagem diz: Você está tentando deixar o Swarm a partir de um nó manager. Removendo o último manager (Pois no caso criamos apenas um) apagará todo o estado atual do Swarm.
        
        Ou seja, todas as nossas configurações serão perdidas...Mesmo assim ele ainda sugere o comando "--force" para o caso de ser isso mesmo que queremos fazer.

            docker@vm1:~$ docker swarm leave --force
            Node left the swarm.

        Realizar este passo acarretará na construção do Zero do nosso Swarm, mas é importante para ver o que pode acontecer em algum momento se não tomarmos conta e não observarmos o estado
        do nosso Swarm desde o início.

        Agora que não temos um nó manager, as nossas workers estão perdidas, por quê? Porque como esse cara não faz parte do nosso Swarm mais, se eu fizer qualquer comando aqui respectivo ao 
        antigo Swarm, por exemplo um "dsocker node ls", ele irá dizer que este nó não é nenhum Swarm manager e nem faz parte de um Swarm, faz todo sentido...

            docker@vm1:~$ docker node ls
            Error response from daemon: This node is not a swarm manager. Use "docker swarm init" or "docker swarm join" to connect this node to swarm and try again.

        Eu não consigo mais executar comandos do escopo do Swarm nesta máquina agora, não faz parte mais de um Swarm. Significa que se eu tentar subir algum serviço novo, por exemplo, 
        "docker service create -p 8080:3000 aluracursos/barbearia"....

            Error response from daemon: This node is not a swarm manager. Use "docker swarm init" or "docker swarm join" to connect this node to swarm and try again.

        Não vamos conseguir porque esse cara ainda é um worker, assim como a nossa vm3. Significa que nós não vamos mais conseguir ler e nem alterar nenhuma informação do nosso Swarm, porque
        o nosso único manager, o único cara que era responsável por isso não existe mais.

        Porém uma coisa interessante é que o serviço nos outros dois nós (vm2 e vm3) ainda está executando, podemos verificar no browser. Não funciona mais apenas na vm1.
        O que isso quer dizer? Em certo ponto é uma coisa boa, pois, por mais que agora não tenhamos mais a administração do nosso Swarm, todos os serviços que lá estavam continuam funcionando
        sem nenhum problema, por mais que não tenha mais um manager. O lado negativo é que não conseguimos mais alterar o estado do nosso Swarm e não conseguimos mais ver nenhuma informação.

        Veremos agora como contornar este problema...

    
    Como fazer backup do Swarm:

        Como poderíamos ter evitado que toda a configuração do Swarm desaparece-se? Como conseguiremos fazer o backup do nosso Swarm de maneira correta?
        Executamos o comando para fazer as outras máquinas deixarem o Swarm também...

            $ docker-machine ssh vm3 /depois/ $ docker swarm leave
            $ docker-machine ssh vm2 /depois/ $ docker swarm leave

        Agora tudo o que tinhamos no Swarm foi apagado...

        Vamos recriar o Swarm do 0 agora...Na vm1 executaremos o comando:

            $ docker swarm init --advertise-addr 192.168.99.106 

            Agora entrar em cada um dos workers e adicionar ao Swarm com o código - 
                (docker swarm join --token SWMTKN-1-2azgwuekfzuktad34274gc322vavuwqe2p4xwohncm47f7c8p0-3bygfhoibe6mnbgg3c9xb26y6 192.168.99.106:2377)

        Agora vamos recriar o serviço à partir da máquina 1...

            $ docker service creater -p 8080:3000 aluracursos/barbearia

            ipqucvqhh4bev9dlccenkog0z
            overall progress: 1 out of 1 tasks
            1/1: running   [==================================================>]
            verify: Service converged

            Serviço convergiu, verificamos com:

            $ docker service ls

            ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
            ipqucvqhh4be        blissful_lamarr     replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

        Como é que podemos fazer um backup do nosso Swarm em determinados momentos para evitar que alguma tragédia aconteça e tenhamos um ponto de retorno para não ter que fazer tudo do Zero
        novamente? Poderíamos ter feito o seguinte, copiado todos os nossos logs, todo o nosso conteúdo do Swarm para uma outra pasta de backup.

        E aonde fica todos esse conteúdo que queremos fazer backup? Poderíamos ver na documentação, mas temos já em mãos, ele fica na pasta "/var/lib/docker/swarm".
        Se tentarmos com o usuário comum teremos o seguinte erro:

            docker@vm1:~$ cd /var/lib/docker/swarm
            -bash: cd: /var/lib/docker/swarm: Permission denied

        Teremos que elever a permissão com "sudo su" ou "sudo -s", agora sim conseguiremos entrar na pasta...

            docker@vm1:~$ sudo -s
            root@vm1:/home/docker# cd /var/lib/docker/swarm
            root@vm1:/var/lib/docker/swarm#

        E agora se usarmos um "ls" ele nos mostra todos os conteúdos, tudo que está sendo gerenciado pelo nosso Swarm...
        
            root@vm1:/var/lib/docker/swarm# ls
            certificates       raft               worker
            docker-state.json  state.json
        
        ...significa que queremos fazer o que? Queremos copiar toda esta pasta para
        onde? Para algum outro lugar de onde possamos recuperá-la depois...

        Por exemplom, vamos copiar toda essa pasta...Voltaremos para "home", faremos um "cp", que é o comando de cópia no terminal. Utilizaremos...

            root@vm1:/home# cp -r /var/lib/docker/swarm/ backup

        Acima vemos uma cópia do comando utilizado para copiar o conteúdo da pasta 'swarm' que se encontra em '/var/lib/docker', para a pasta backup (crida no comando) que está na home.

        Como tudo isso acima está em uma pasta, utilizamos o parâmetro '-r' de cópia recursiva, que copia a pasta e tudo que ela tem dentro...
        Se utilizarmos o 'ls' agora na pasta backup que está em 'home', veremos o conteúdo dentro dela...

            root@vm1:/home/backup# ls
            certificates       raft               worker
            docker-state.json  state.json

        Agora, vamos imaginar o que vai acontecer no nosso caso lá no Swarm, tudo queimar novamente, vamos dizer que o Swarm todo parou de funcionar, como é que conseguiríamos recuperar ele
        agora? Vamos repetir o nosso erro, vamos executar o comando "docker node rm", vamos remover os dois nós adicionados ao Swarm, "vm2 --force", 

            root@vm1:/home/backup# docker node rm vm2 --force

            root@vm1:/home/backup# docker node rm vm3 --force

            Agora se dermos um...

            $ docker node ls

            Só temos a vm1, e agora utilizaremos o:

            $ docker swarm leave --force

        Se formos agora em "/var/lib/docker/swarm", o que irá acontecer?

            $ cd /var/lib/docker/swarm

        Estará vazio...então todo aquele controle que tínhamos foi perdido, por isso que no momento em que criamos um novo Swarm ainda a pouco, não tinha nada como tinha antes, tudo tinha
        sido apagado...

        E agora o que queremos fazer? Queremos criar um novo swarm usando as configurações que salvamos no nosso backup, então vamos voltar aqui para a nossa home e vamos fazer o seguinte...
        Vamos utilizar o comando "cp" de novo junto com o "-r", e vamos copiar todo o conteúdo que está dentro de "backup/*", para onde? Para "/var/lib/docker/swarm"

            root@vm1:/home# cp -r /backup/* /var/lib/docker/swarm/

        E agora o que precisamos fazer? Já copiamos todo o conteúdo para lá de novo, agora precisamos recirar o nosso swarm, e ele vai utilizar essas configurações que estão copiadas dentro
        dessa pasta, mas se eu for ali no terminal e simplesmente fazer um "docker swarm init --advertise-addr 192.168.99.106", qual é o IP que ele irá sugerir?

        Ele não necessariamente vai utilizar essas configurações, eu preciso forçar ele a recarregar todas as configurações que estão dentro desse caminho aqui "/var/lib/docker/swarm"...
        E como é que forçamos ele a usar essas configurações? Vamos colocar uma outra flag antes, que se chama "--force-new-cluster" e agora dando um 'Enter', o que ele faz? Ele recria o Swarm...

            root@vm1:/home# docker swarm init --force-new-cluster --advertise-addr 192.168.99.106
            Swarm initialized: current node (j8owlnxj5wmrz3v8wq6dtwahp) is now a manager.
            
            To add a worker to this swarm, run the following command:
            
                docker swarm join --token SWMTKN-1-2azgwuekfzuktad34274gc322vavuwqe2p4xwohncm47f7c8p0-3bygfhoibe6mnbgg3c9xb26y6 192.168.99.106:2377
            
            To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

        Se voltarmos nas nossas páginas e dermos um 'F5', tudo estará funcionando normalmente...Se utilizarmos também o "docker node ls", dentro da nossa manager, o que ele mostrará? Que nós já
        temos os nossos nós, a nossa vm2 e a nossa vm3 já estão dentro do Swarm de novo.

        Se dermos um "docker service ls"...

            ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
            ipqucvqhh4be        blissful_lamarr     replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

            ...o serviço está rodando, se dermos um "docker service ps ip (Que é o início do id do serviço)"

            ID                  NAME                    IMAGE                          NODE                DESIRED STATE       CURRENT STATE           ERROR                              PORTS
            muj9lor14r1n        blissful_lamarr.1       aluracursos/barbearia:latest   vm1                 Running             Running 5 minutes ago
            5fulhdsz10ht         \_ blissful_lamarr.1   aluracursos/barbearia:latest   vm1                 Shutdown            Failed 5 minutes ago    "No such container: blissful_l…"

            ...ele vai mostrar uma coisa bem interessante...Que tem uma tarefa, ele tentou subir uma vez e falhou, na segunda ele conseguiu rodar esse serviço, que é a nossa barbearia da Alura,
            inclusive na vm1, lembra o que foi dito? A nossa vm manager não é o ideal para carregar e executar container, nós ainda vamos estudar isso, como isso pode ser evitado.

            Então é isso...Acabou de criar o Swarm com os workers desejados, faça um backup, para caso de algum problema que faça o Swarm parar de funcionar termos o backup dele...Antes que
            percamos todos os nossos managers ou alguma coisa mais absurda, tenhamos sempre o conteúdo mais atual dele e só precisa recriar ele com o:

                $ docker swarm init --force-new-cluster

                ...copiando nosso backup para dentro da pasta "/var/lib/docker/swarm"

    Criando mais managers:

        De quais outras formas poderíamos ter evitado aquele desastre que aconteceu? É claro que pudemos evitar isso fazendo o backup, mas como poderíamos ter evitado de ter que usar o nosso
        backup de maneira tão rápida? Será que podemos deixar o nosso Swarm mais robusto? Que ele seja mais inteligente, que ele consiga lidar melhor com essas falhas? Porque veja bem, tínhamos
        um manager até então, será que poderíamos ter mais um, dois, três, cinco, dez? Podemos!

        Como faremos isso? Como podemos deixar o nosso Swarm mais inteligente? Menos suscetível a erros na verdade? Removeremos novamente os nós do Swarm com o comando:

            $ docker swarm leave --force

            Entraremos nas vm's 2 e 3 com o comando "$ docker-machine ssh vmx" e removeremos as máquina virtuais do Swarm com o comando acima.

        E agora o que vamos fazer? Mais uma vez iremos recriar o Swarm na vm1 (Não se esqueça que ela também deve ser removida do Swarm ou então um erro será exibido):

            $ docker swarm init --advertise-addr ...

            e iremos colocar 192.168.99.106, clicaremos no 'Enter'. Ele recriou aqui mais uma vez o swarm, reparem que não usamos o force-new-cluster, não queremos usar nenhum backup, estamos
            criando ele do zero.

        E agora ele me dá o comando de adicionar um worker, mas agora queremos adicionar um manager, e reparem, ele já até dá a resposta aqui no final...Tínhamos aquele comando:

            $ docker swarm join-token worker --- para poder adicionar um worker ao nosso Swarm.

        Se quisermos adicionar um manager, é só botar "docker swarm join-token manager", e ele vai dar o output aqui para nós mostrando o quê? O comando para adicionar um manager ao Swarm.
        Reparem que o comando é o mesmo, toda a primeira parte pelo menos, até o último hífen. O que ele troca no final é só o token que vai definir se o nó vai entrar como um worker ou como um
        manager no Swarm.

            $ docker swarm join-token worker  -> docker swarm join --token SWMTKN-1-1cl89jndiy1wtx413is0ksie8w0j47du6xwhsh0qz89jz5gvkv-22u0i4ztgldudknywbx7xoup5 192.168.99.106:2377
            $ docker swarm join-token manager -> docker swarm join --token SWMTKN-1-1cl89jndiy1wtx413is0ksie8w0j47du6xwhsh0qz89jz5gvkv-6epka2ougox54z878xkfa38k2 192.168.99.106:2377
            
        Agora pegamos esse comando e colamos nas nossas vm2 e vm3. A saída deverá ser: "This node joined a swarm as a manager."
        Agora temos o que? Agora temos um Swarm com três managers, mas precisamos também adicionar os nossos worker para fazer o nosso trabalho duro.

        Por mais que tenhamos visto que o nosso Swarm consegue atribuir container, serviços aos nossos managers, precisamos ainda ter os nossos worker porque vamos entender mais para frente
        qual é a definição, do que devemos ou não colocar no manager.

        Mas vamos lá, precisamos agora criar mais workers. Para criá-los utilizaremos o comando:

            $ docker-machine create -d virtualbox vm4
            $ docker-machine create -d virtualbox vm5

        Se formos na nossa vm1 e executarmos o comando "docker node ls", ela vai exibir algumas informações para nós.

            Obs: No curso o instrutor quis ensinar como melhorar a visualização logo em seguida, já que estava utilizando o terminal do Windows.
                Como estou utilizando o VSCode desde o ínicio, não tive este problema.

        Veremos como exibir apenas alguns atributos, como exemplo pegaremos apenas o 'HOSTNAME' e o 'MANAGER STATUS'...Utilizaremos o comando:

            $ docker node ls --format "{{.Hostname}} {{.ManagerStatus}}"

            Teremos a seguinte saida:

            vm1 Leader
            vm2 Reachable
            vm3 Reachable

        Podemos ver que a nossa vm1 agora ainda é a líder, mas reparem que a nossa vm2 e a nossa vm3 são 'Reachable', ou seja, elas não tem a ausência de informação que tinham anteriormente, 
        quando eram vazias, eram workers, agora elas são manager.

        Vamos adicionar agora as nossas vm4 e vm5 aqui no nosso Swarm. Vamos acessá-las com "docker-machine ssh vm4" e "docker-machine ssh vm5" e vamos adicioná-las como worker e só para
        mostrar que podemos, vamos ir na vm3 e vamos executar o comando:

            $ docker swarm join-token worker

        E agora como ela é uma manager, ela vai conseguir dar esse output para nós que é bem legal:
        
        docker swarm join --token SWMTKN-1-1cl89jndiy1wtx413is0ksie8w0j47du6xwhsh0qz89jz5gvkv-22u0i4ztgldudknywbx7xoup5 192.168.99.108:2377

            Copiamos o comando, agora iremos até a vm4 de fato, e vamos executá-lo...Faremos o mesmo na vm5. Se executarmos na nossa vm1 novamente o comando para formatar tudo certinho, ele
            vai exibir vm1, vm2, vm3, vm4 e vm5, mas só que mais uma vez, só com aquela formatação de exibir apenas o hostname e o manager status:

                $ docker node ls --format "{{.Hostname}} {{.ManagerStatus}}"

                Saída:

                vm1 Leader
                vm2 Reachable
                vm3 Reachable
                vm4
                vm5

        Então se não tem nenhuma informação da vm4 e da vm5, significa que elas são workers, se ele está exibindo esse reachable na vm2 e na vm3 significa que elas são managers, e como ele
        está exibindo esse leader na vm1, significa que além de manager, ela é uma líder, ou seja, ela é a liderança máxima que tem dentro desse Swarm.

        Mas o que podemos pensar agora? Agora que temos um 'Leader' e mais dois membros managers, 'Reachable'...O que poderia acontecer? Vamos parar e pensar no seguinte, temos a nossa vm1,
        e se ela parar de funcionar, o que vai acontecer? Será que vamos ter que decidir entre a vm2 e a vm3 quem vai ser o nosso novo líder? Exatamente!

        E como isso vai acontecer? Vamos fazer um teste, vamos na vm1 e mais uma vez, vamos simular uma catastrofe, vamos executar um "docker swarm leave --force", removendo ela do Swarm de 
        forma abrupta. Se formos agora na nossa vm2 ou na nossa vm3 que também são managers ainda, reparem que nós ainda conseguimos dar um "docker node ls", ou seja, nós ainda temos status, 
        conseguimos visualizar todas as informações do nosso cluster.

        Se utilizarmos novamente o "docker node ls --format", só para ficar mais apresentável, colocarmos "{{.Hostname}}" e depois "{{.ManagerStatus}}", ele vai exibir que a vm1 está com 
        estado de 'Unreachable', ou seja, está inacessível, a vm3 ainda é manager porque ela é 'Reachable'.

        E agora a vm2 é a nossa nova líder, ou seja, nós sempre temos um novo líder, a pergunta que fica é: "Como esse cara foi eleito como líder?", veremos nos próximos capítulos sobre o
        consenso, como o Docker faz essa votação.

    
    Algoritmo de consenso RAFT:

        O que acabou de acontecer, no que diz respeito da escolha do novo Leader? Foi feita uma eleição e foi decidido no caso aqui da minha máquina, do nosso Swarm que a vm2 é a nova líder,
        pode ser que na sua máquina tenha sido eleita a vm3, se você estiver fazendo isso que nem estou fazendo, mas a ideia aqui é ver que aconteceu uma eleição que vai variar o resultado
        conforme o nosso sistema, conforme a nossa máquina.

        Mas a pergunta que fica é: Como foi feita essa eleição? Quem fez essa eleição? Será que ela sempre funciona? Para começarmos a responder isso, precisamos entender que foi aplicado na
        verdade um algoritmo de consenso e esse algoritmo de consenso é um cara chamado RAFT, o simbolo usado no exemplo é como uma canoa, uma jangada, mas o que importa mesmo não é o símbolo
        e sim o que ele faz e como ele faz.

        Sempre temos um só líder, e o que acontece quando este líder falha? O líder falhou e a partir de agora, entre todos os outros managers que tem o nosso Swarm, vai ser feita uma votação,
        uma eleição, e cada um vai votar em quem eles querem que seja o novo líder.

        No meu caso, foi eleita a vm2, mas no momento dessa votação, que eles estão "depositando" o voto, poderia ter sido eleita tanto a vm2 quanto a vm3, o que improta é, temos algumas
        regrinhas, temos algumas restrições para que o consenso possa ser atingido.

        E quais são as regras para que o RAFT consiga funcionar? Na verdade é bem simples, precisamos do quê? Precisamos ter o número máximo de falhas que vamos conseguir aguentar e
        consequentemennte, vamos ter um quórum mínimo para que a eleição seja feita. Porque vamos imaginar, se abrissemos a votação e tivessemos só uma pessoa, não faria sentido ter uma votação.

        Então vamos lá, o que acontece? Tínhamos três managers, tanto faz se um era líder ou não, o que importa é que no fim das contas todos os três são managers, mas o que acontece é que, 
        vamos suportar no caso, N - 1 falhas, no caso aqui nosso N é o número de managers, então vamos suportar ((3 - 1) / 2) (Três menos um igual a dois, dividido por dois), então ele só 
        suporta uma falha nesse caso aqui que temos três managers, e conseguentemente, o nosso quórum mínimo vai ser N dividido por 2 que é 1, resto 1.
        
        E aqui no caso vamos ter N sobre 2 mais 1, ou seja, nosso quórum tem que ser no mínimo 2, então estamos já no nosso limite nesse caso aqui, se tivéssemos cinco managers o que aconteceria?
        Suportaríamos 5 - 1 = 4, dividido por 2 = 2 falhas, e o nosso quórum mínimo teria que ser N sobre 2 daria 2 mais 1 = 3, que é o nosso quórum ali para ter nossa eleição.

        Número de falhas = (N - 1) / 2 (Onde N é o número de máquinas)
        Quórum = N / 2 (+ 1) (Onde N é o número de máquinas...E (/) para dividir e ignorar o resto)

        Então repara, quanto mais managers temos, maior a nossa capacidade de suportar falhas e o nosso quórum vai estar lá sempre acompanhando esse número de falhas, por exemplo, por que não
        adicionamos logo então 300 milhões de managers no nosso Swarm, o problema estará resolvido? Porque na verdade, quanto mais managers adicionamos no nosso Swarm, maior é o nosso tempo
        de processamento de leitura e escrita, e isso vai reduzir o nosso desempenho cada vez mais.

        Então quanto mais managers nós temos, vamos ter um desempenho menor nesse longo prazo adicionando mais managers, e o que acontece? O próprio Docker, a própria empresa em si, recomanda que
        nós sempre tenhamos três, cinco ou sete managers, porque tendo esses números ímpares, nós conseguimos sempre ter um número a mais ali para segurar o nosso Swarm.

        E sempre vai ter um consenso, no caso 9 ainda é aceitável, mas nunca mais do que 10, porque aí já começamos a ter um número muito elevado de managers e vamos começar a ter uma taxa de
        leitura e escrita muito grande, nosso desempenho vai começar a cair.

    
    Nesta aula, aprendemos:

        - Que nós managers são primariamente responsáveis pela orquestração do swarm
        - A importância e como realizar o backup do swarm
        - Que podemos ter mais de um nó manager no swarm
        - A importância do Leader dentro do swarm
        - Como é feita a eleição de um novo Leader em caso de falhas
        - Os requisitos para funcionamento do RAFT


    Questões aula 03:

        01 - Caso nosso único manager pare de funcionar, podemos ter problemas. O que acontecerá com o nosso swarm em caso de perda do manager?

            Selecione 2 alternativas

            R1: As tarefas em execução em outros nós serão mantidas sem problemas.

            Alternativa correta! A ausência do manager não afetará as tarefas de outros nós.

            R2: Não conseguiremos mais executar comandos de leitura e/ou criar novos serviços.

            Alternativa correta! Com a ausência do manager, não teremos mais nós capazes de executar comandos administrativos.

        
        02 - É muito importante fazermos backup de todo o nosso swarm para evitarmos desastres. Por padrão, em qual diretório fica armazenado o conteúdo do nosso swarm?

            Selecione uma alternativa

            R: /var/lib/docker/swarm

            Alternativa correta! Nesse diretório temos todas as configurações de estado do nosso swarm.


        03 - Ao utilizarmos o comando docker node ls, como podemos identificar quais nós são managers dentro do nosso swarm?

            Selecione uma alternativa

            R: Basta olhar a coluna Manager Status e ver quais nós tem o valor Reachable ou Leader.

            Alternativa correta! Nós com esses status são managers dentro do nosso swarm.

        
        04 - Vimos que em caso de falhas do Leader do swarm, temos uma eleição entre os nós managers para definir o novo líder. Se tivéssemos um swarm com 7 nós managers, qual seria o nosso 
            quórum necessário e número máximo de falhas para realização da eleição?

            Obs: Caso não lembre das regras, reveja o último vídeo a partir dos 4:00.
        
            Selecione uma alternativa

            R: 4 para o quórum e 3 falhas no máximo.

            Alternativa correta! Como nosso quórum é (N / 2) + 1 e o número máximo de falhas é (N - 1) / 2, temos o valor esperado.


Aula 04: Separando as responsabilidades -------

    Readicionando um manager:

        Então vamos fazer o que agora? Vamos entender como vamos diviri as responsabilidades aqui dentro do nosso Swarm. Porque viemos falando desde o início que papel dos nossos managers
        é fazer orquestração e ler operação de leitura e escrita dentro do Swarm enqaunto os workers carregam e executam os containers, as tarefas, agora por assim dizer.

        Mas vamos parar e pensar no seguinte, em certo momento do nosso curso foi alocada uma tarefa dentro da vm1, ou seja, o próprio Swarm não se opôs de colocar uma tarefa para ser executada
        dentro de um manager, ao invés de colocar ela em um worker, mas a ideia é que evitemos que isso aconteça e nós só vamos querer rodar determinados containers, determinadas tarefas quando
        quisermos que sejam executadas e não que dê essa liberdade de colocar por exemplo alguns serviços quaisquer dentro daquela manager.

        E como é que faremos isso? Vamos voltar no nosso terminal e vamos fazer o seguinte, vamos dar um "docker node ls" mais uma vez e utilizar de novo o "{{.Hostname}}" e também o 
        "{{.ManagerStatus}}" para ver nossa situação.

            $ docker node ls --format "{{.Hostname}} {{.ManagerStatus}}"

            Saída:
                vm1 Unreachable
                vm2 Reachable
                vm3 Leader
                vm4
                vm5

            A nossa vm1 ainda está unreachable, vamos adicionar ela novamente, quais seriam os passos? Vamos primeiro utilizar o "docker node rm vm1"

                    Saída: Error response from daemon: rpc error: code = FailedPrecondition desc = node o6npnn8j8tf10169sxlee4d5c is a cluster manager and is a member of the raft cluster. 
                                It must be demoted to worker before removal

            Que erro é esse? Ele diz que: "Este nó é um manager do cluster e é um membro do raft cluster", ou seja, mesmo que ele esteja "Unreachable", ainda é um membro.

        Ele deve ser "demoted" para o worker antes de ser removido, ou seja, ele tem que ser rebaixado, abaixar o nível dele para que ele seja removido, e como é que fazemos isso? Antes de
        mais nada vamos utilizar o "docker node demote vm1"...

            Saída: Manager vm1 demoted in the swarm.

        ...agora a vm1 por mais que ela ainda esteja "Unreachable", ela é uma worker comum dentro do nosso Swarm, e podemos remover ela agora sem nenhum problema.

        Vamos utilizar o comando para remove-lá:

            $ docker node rm vm1

        Agora para tornar nosso cluster ao estado original, vamos readicionar a vm1 como manager, para a dica utilizamos o comando "docker swarm join-token manager". Acessamos via ssh a vm1
        e colocamos o comando "docker swarm join --token SWMTKN-1-1cl89jndiy1wtx413is0ksie8w0j47du6xwhsh0qz89jz5gvkv-6epka2ougox54z878xkfa38k2 192.168.99.107:2377" e adicionamos
        "--advertise-addr 192.168.99.106" só para mantermos o nosso IP original lá.

            Saída: This node joined a swarm as a manager.

            Utilizamos o comando:

                $ docker node ls

                Teremos a saída:

                    ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
                    nhuzr9tiek2za2q26dcbwuwdl *   vm1                 Ready               Active              Reachable           19.03.12      
                    in3t87hcu77qnk13ln538rg29     vm2                 Ready               Active              Reachable           19.03.12
                    mr4jjwio2une2dn51od448g9q     vm3                 Ready               Active              Leader              19.03.12
                    jh0vk0wrgiruzrejjhfor8l3u     vm4                 Ready               Active                                  19.03.12
                    rzmb7ke49zeocehp5liaq97ez     vm5                 Ready               Active                                  19.03.12

        Tudo voltou a funcionar! A nossa Leader agora é a vm3. O que faremos agora? Vamos voltar para a nossa vm2, vamos questionar o seguinte, se nós subirmos algum Serviço aqui com
        o "docker service create" mais uma vez, "-p 8080:3000 aluracursos/barbearia", o que vai acontecer aqui? Ele vai criar a nossa task, o nosso serviço.

        E vamos ver o que vai acontecer...

            $ docker service create -p 8080:3000 aluracursos/barbearia

            Saída:

                jb8m98lf24u8rg7p9svtvrg9b
                overall progress: 1 out of 1 tasks
                1/1: running   [==================================================>]
                verify: Service converged

        Utilizamos mais uma vez o "docker service ls"

            Saída:

                ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
                jb8m98lf24u8        kind_feynman        replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

                ...temos ele aqui rodando, o ID, o início é "jb", vamos utilizar "docker service ps jb" (Só precisamos colocar os primeiros dois dígitos do ID do serviço).

                Saída:

                    ID                  NAME                IMAGE                          NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
                    p82rpw90dm5v        kind_feynman.1      aluracursos/barbearia:latest   vm1                 Running             Running 4 minutes ago

                Ele subiu aonde? Na vm1, também podemos utilizar o "--format" aqui, e fazermos o que agora? Colocamos o que? O que queremos ver! No caso o "{{.Node}}".

                    $ docker service ps jb --format {{.Node}}

                    Saída:

                        vm1

        Então sabemos que esse servilom essa tarefa, foi atribuída para vm1, mas a vm1 é uma manager, e é aquela questão que acabamos de falar...Será que devemos evitar que quaisquer serviços,
        quaisquer tarefas sejam executadas dentro da manager? E mais, como é que evitamos que isso aconteça? Como é que só rodamos serviços que nós queemos que rode dentro de manager no momento
        em que queremos?

    
    Restringindo nós:
        
        Como é que podemos evitar que serviços sejam rodados na manager? Porque veja bem, o serviço que subimos ainda pouco está sendo executado em manager porque por padrão Docker, o Swarm
        permite que serviços sejam executados dentro da manager.

        Mas a ideia é restringirmos os serviços que queremos que sejam executados em managers ou não, mas vamos ver na próxima aula daqui a pouco em quais momentos devemos ou não executar um
        serviço dentro de uma manager, quais serviços devem fazer isso. Mas antes disso, precisamos aprender como restringimos o comportamento de um serviço ou de um nó para ele ser executado
        dentro de um manager ou não.

        Vamos começar fazendo aquele velho procedimento, vamos para mais uma vez o serviço que está em execução, e como paramos um serviço? Podemos listar o serviço, criar serviço, podemos
        remover serviço. Vamos utilizar "docker service rm", e para removermos basta passar aqui o ID do serviço que ele quer remover.

        Mas uma outra possibilidade é, podemos remover todos os serviços de uma vez, só para vermos como faz isso, utilizando a técnica que vimos lá no curso anterior (de Docker) que é passar
        dentro de parênteses, que é seguido de um cifrão o comando que queremos usar com uma entrada para esse comando aqui.

        Qual o comando que eu quero usar como entrada para o comando de remover? Eu quero que ele use como entrada o ID de todos os serviços que estão em execução, e como eu pego os serviços
        que estão em execução? Com o "docker service ls", para eu pegar só os IDs, executamos um "docker service ls -q", e damos um Enter aqui, e ele remove.

            $ docker service rm $(docker service ls -q)

            Podemos utilizar um "docker service ls" em seguida, para ver se realmente todos os serviços foram removidos.

        Vamos lá então se subirmos mais uma vez o serviço, será teimosia, o que poderá acontecer?

            $ docker service create -p 8080:3000 aluracursos/barbearia

            Ele vai começar a construir, mas uma vez vai tentar fazer os testes de convergência...

                oo3fmgtoiczzl5uvtzt04rfhm
                overall progress: 1 out of 1 tasks 
                1/1: running   [==================================================>]
                verify: Service converged

            ...se tudo der certo ele vai contruir, se dermos então, o "docker service ls"...

            ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
            oo3fmgtoiczz        nervous_dhawan      replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

            ...depois "docker service ps oo"

            ID                  NAME                IMAGE                          NODE                DESIRED STATE       CURRENT STATE                ERROR               PORTS
            nihqij3fvo7r        nervous_dhawan.1    aluracursos/barbearia:latest   vm2                 Running             Running about a minute ago

        Vemos que ele o serviço foi alocado com esse ID "oo3fmgtoiczz", quando utilizamos o segundo comando pudemos verificar que a tarefa foi gerada na vm2, e a vm2 também é um manager.

        Temos duas possibilidades de como evitar que algum serviço seja executado dentro de um determinado nó, vamos ver a primeira, como é que podemos chegar e falar "Eu quero que a minha vm2,
        a minha manager, não rode nenhum serviço", eu vou restringir o meu nó para que ele não execute nenhum serviço, e só faça papel administrativo.
        
        E como restrinjo a capacidade desse nós de executar serviço? Se dermos um "docker node ls" aqui, o que vai acontecer? Ele vai listar os nossos nós aqui, e mais, ele está mostrando essa
        coluna aqui de disponibilidade, nesse 'AVAILABILITY', e está mostrando o status de ativo para todos os nós, inclusive a nossa vm2.

        E o que significa esse availability que vimos lá na primeira aula? Significa que esse nós está disponível para rodar serviços e se eu quero tornar ele indisponível para rodar qualquer
        tipo de servilo, eu preciso trocar esse status para indisponível.

        E como é que vamos fazer isso? Temos um comando bem interessante que é o "docker node update", ou seja, vamos atualizar o estado de um nó, e o que queremos atualizar? O availability,
        e podemos atualizar ele com esse "--availability" aqui, e agora colocamos o status que quereos colocar nesse estadom eu quero que ele esteja com esse status de "drain", que é o Status
        que por padrão não vai permitir que algum serviçlo seja executado dentro desse nó.

        Por fim, eu preciso informar qual é o nó que eu quero atualizar, porque esse comando aqui, pode ser executado dentro de qualquer manager, e por fim, eu preciso falar q  ue eu quero
        atualizar a minha vm2, ele deu um output aqui na própria vm2, então significa que tudo correu bem.

            $ docker service ls  
            
                ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
                oo3fmgtoiczz        nervous_dhawan      replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

            $ docker service rm oo

            $ docker node update --availability drain vm2
            
            $ docker node ls
                ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
                q5duspz3ugglfw664ln3xr18n     vm1                 Ready               Drain               Leader              19.03.12      
                in3t87hcu77qnk13ln538rg29 *   vm2                 Ready               Drain               Reachable           19.03.12      
                mr4jjwio2une2dn51od448g9q     vm3                 Ready               Active              Reachable           19.03.12
                jh0vk0wrgiruzrejjhfor8l3u     vm4                 Ready               Active                                  19.03.12
                rzmb7ke49zeocehp5liaq97ez     vm5                 Ready               Active                                  19.03.12
    
        No caso, logo acima, utilizamos mais uma vez "docker node ls", está com o status de drain agora. Significa que, se dermos um "docker service ps" para ver como está essa tarega que
        estava neste nó vm2, ele foi realocado agora para outro nó, não mais aqui na nossa vm2.

        Ou seja, so movemos o problema de lugar, estamos meio que em uma caça ao rato agora, não queremos mais que seja rodado na vm2, mas ele tentou e fez na vm3, então para cada novo nó
        que tivermos no nosso Swarm que não queremos que seja executada como um serviço nele, vamos ter que ficar nessa brincadeira de ficar mudando o status dele para drain.

        E mais, com essa mudança, tornamos o nosso nó completamente incapaz de rodar algum serviço e como falamos ainda a pouco, a ideia não é que nenhum serviço nunca vá ser executado dentro
        de um manager, mas com o drain, tornamos esse nó incapaz de rodar qualquer serviço.

        Essa foi a primeira possibilidade, vamos coloca o status destes nós de volta, "--availability" "Active" "vmx". 

            $ docker node update --availability Active vm"X"

        Se utilizarmos de novo o "docker node ls" veremos que todos os nós estão como Active novamente...Porém se utilziarmos de novo o "docker service ls" e "docker service ps 'id serviço'"
        veremos que a tarefa continua no nó vm4, não é porque reestabelecemos a disponibilidade dos nós vm1 e vm2 que o serviço volta para onde estava.

        Se formos no navegador verificar, veremos que o serviço está rodando sem nenhum problema. Vimos a primeira possibilidade que é restringindo nosso nó, criamos um limite ali para dentro
        do nosso nó, para que ele não rode serviços. A outra possibilidade que é um pouco menos invasiva.

    
    Restringindo serviços:

        Vamos então agora à nossa segunda possibilidade, estamos começando logo de onde paramos no ponto anterior. O serviço teve a nova tarefa criada na vm4, mas a pergunta que fica é:
        Como eu posso falar agora para o serviço que eu não quero que ele seja executado em determinados tipos de nós, ou seja, como é que eu restrinjo um serviço e não mais um nó? Será que
        dá para fazer isso? É claro que sim!

        Mas como é que podemos atualizar uma restrição, como é que podemos impor um limite a um serviço? Assim como tínhamos o nosso "docker node uptate" para atualizar o estado de um nó,
        também temos o nosso "docker service update" para atualizar o estado de um serviço.

        E o que precisamos fazer? Precisamos adicionar uma nova restrição a esse serviço, e como é que faremos? Se nós queremos fazer como mexemos na "--availability" de um nó, adicionaremos uma
        restrição, uma  "--constraint-add", e qual é a restrição que queremos? Queremos que este serviço, que criamos ainda a pouco, só rode em nós que tenham o papel de worker no nosso Swarm.
        Então adicionamos qual é a regra desta restrição logo em seguida com "node.role"

        E quais nós tem esse papel? A nossa vm4 e a nossa vm5, não precisamos listar aqui para ele em qual hostname, em qual nó por nome queremos fazer isso, basta explicitarmos o papel desse nó
        dentro do Swarm, então queremos que ele funcione apenas dentro de nós que tenham um papel de "worker", por isso colocamos em forma de comparação "==worker" (Encostado em "node.role")...

            $ docker service update --constraint-add node.role==worker "ID serviço"

        Se dermos um 'Enter' agora, ele irá atualizar o estado do serviço...

            4d
            overall progress: 1 out of 1 tasks
            1/1: running   [==================================================>]
            verify: Service converged

        Se utilizarmos o comando "docker service ps" novamente, passar o ID do serviço, veremos que ele foi criado em outro nós que é apenas worker, a vm4...

            ID                  NAME                      IMAGE                          NODE                DESIRED STATE       CURRENT STATE             ERROR                         PORTS
            oudob38vx2r1        intelligent_chaum.1       aluracursos/barbearia:latest   vm4                 Running             Running 25 seconds ago
            4c3809jhtmty         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Shutdown 26 seconds ago
            0yukvri4q7zy         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Failed 10 minutes ago     "task: non-zero exit (137)"
            198nayz45gli         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Failed 18 minutes ago     "task: non-zero exit (137)"
            h9aq2arwxrtk         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm4                 Shutdown            Failed 40 minutes ago     "task: non-zero exit (255)"

            Obs: Já tinha encerrado na vm4 anteriormente o serviço para fazer o teste, e atualmente estava rodando na vm2 que é um manager, repare pelo histórico.

        Vamos ver se o serviço está rodando normalmente, vamos pegar o IP e colocar no browser...Caso não lembre o IP, basta sair do nós e executar um "docker-machine ls"...
        Bom, vimos que tudo continua funcionando perfeitamente. Agora podemos fazer o teste derrubando o serviço na vm4 para ver em qual nó o serviço será levantado novamente...
        Vamos até a vm4, executar um "docker container ls", utilizaremos o ID do container para derrubá-lo junto com o comando "docker container rm "ID container" --force".

        Vamos ver em qual nó foi levantado o serviço que derrubamos junto com o container. Primeiro precisamos ir para um nó que seja manager...No caso vm1, vm2 ou vm3...
        Então executar os comandos:

            $ docker service ls

            Pegar pelo menos o início do ID...

            $ docker service ps "id"

            ID                  NAME                      IMAGE                          NODE                DESIRED STATE       CURRENT STATE             ERROR                         PORTS
            u5p5002sml28        intelligent_chaum.1       aluracursos/barbearia:latest   vm5                 Running             Running 46 seconds ago
            oudob38vx2r1         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm4                 Shutdown            Failed 2 minutes ago      "task: non-zero exit (137)"
            4c3809jhtmty         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Shutdown 12 minutes ago
            0yukvri4q7zy         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Failed 22 minutes ago     "task: non-zero exit (137)"   
            198nayz45gli         \_ intelligent_chaum.1   aluracursos/barbearia:latest   vm2                 Shutdown            Failed 30 minutes ago     "task: non-zero exit (137)"

            Podemos observar que o serviço foi realocado no nó "vm5", que também é um worker. Se ficarmos derrubando e levantando containers o serviço ficará alternando entre os nós que são
            apenas workers, de acordo com a restrição e regra que criamos.

        Talvez você esteja se perguntando, "E se essa restrição que utilizamos não for aplicável? E se não tiver nenhuma possibilidade para ele rodar esse serviço? Se colocarmos uma condição
        que ele não consiga atender?"...

        Vamos utilizar, por exemplo, uma restrição que não exista, exemplo "worker123"...

            $ docker service update --constraint-add node.role==worker123 "ID serviço"

            ...é um papel que não existe dentro do nosso Swarm, se utilizarmos este comando acima por exemplo, reparem, ele vai começar a tentar agendar essa tarefa, mas ele vai informar que
            não tem nenhum nó disponível que satisfaça essa restrição entre os cinco nós que temos.

                overall progress: 0 out of 1 tasks
                1/1: no suitable node (scheduling constraints not satisfied on 5 nodes)

        Significa que se formos lá no browser acessar o serviço ele vai ter parado de funcionar, porque o nosso serviço como um todo, a nossa tarefa não está alocada em nenhum nó porque
        nenhum nó está disponível, então sempre temos que tomar cuidado em restringir o nosso serviço, mas também de uma maneira em que nós sempre atendamos isso em algum nó.

        Vimos essas duas possibilidades, tanto restringindo um nó quanto o serviço, e cada caso dependendo da nossa situação, vai variar, será que vale a pena eu restringir o meu nó? Será
        que vale a pena eu restringir o meu serviço? O que vai custar menos e o que vai ser mais fácil de eu manter? É isso!

        Vimos agora como conseguimos restringir também os nossos serviços, não mais os nossos nós, essa técnica é um pouco menos invasiva, mas precisamos entender agora mais um pouco quando
        e quais tipos de serviço vamos executar em um manager, será que realmente vale a pena? Que tipo de serviço podemos fazer isso? Vamos descobrir isso no proximo vídeo.

    Para saber mais:

        No último vídeo vimos como podemos adicionar restrições a um serviço utilizando o comando:

        docker service update --constraint-add
        Utilizamos o comando acima para restringir serviços a funcionarem apenas em nós managers ou workers.
        Porém, também podemos impor outros tipos de restrições, como id, hostname e o próprio role. Vamos ver alguns exemplos!
        
        Caso quiséssemos restringir o serviço de id ci10k3u7q6ti para funcionar apenas em um nó com id t76gee19fjs8, poderíamos utilizar o comando:
        docker service update --constraint-add node.id==t76gee19fjs8 ci10k3u7q6ti
        
        Se o objetivo fosse fazer o serviço rodar apenas em nossa vm4 por exemplo, uma possibilidade seria utilizar:
        docker service update --constraint-add node.hostname==vm4 ci10k3u7q6ti
        
        Por fim, podemos também remover restrições criadas utilizando o comando de atualização passando a flag --constraint-rm. Para remover as duas restrições anteriores:
        docker service update --constraint-rm node.id==t76gee19fjs8 ci10k3u7q6ti        
        docker service update --constraint-rm node.hostname==vm4 ci10k3u7q6ti
        
        Após esse momento, quaisquer novas réplicas criadas para esse serviço poderão ser alocadas sem restrição alguma!

    
    Nesta aula, aprendemos:

        - Como readicionar um manager posterior a uma falha
        - Restringir nós de executarem quaisquer serviços utilizando o docker node update --availability drain
        - Restringir serviços de serem executados em determinados nós utilizando o docker service update --constraint-add


    Questões aula 04:

        1 - No último vídeo, removemos um nó manager do swarm a partir de outro manager. Quais os procedimentos que devemos executar para realizar essa remoção?

            Selecione uma alternativa

            R: Devemos primeiramente rebaixar o cargo do nó com o comando docker node demote e depois removê-lo com o comando docker node rm.

            Alternativa correta! É necessário transformar o nó manager em worker e depois remover.

        2 - Quando o nosso objetivo é restringir o comportamento de nós de um swarm, podemos utilizar o comando:

            docker node update --availability drain

            Qual das alternativas abaixo contém o comportamento esperado do nó sobre o qual foi aplicado esse comando?
            
            Selecione uma alternativa

            R: O nó ficará indisponível para executar tarefas.

            Alternativa correta! Com a disponibilidade em drain, não conseguiremos executar mais tarefas/containers nesse nó.

        
        3 - Podemos também aplicar restrições em serviços utilizando o comando:

            docker service update --constraint-add [outras infos]

            Se quisermos restringir o comportamento para um serviço ser rodado apenas em nós workers, o que mais precisamos informar para o comando acima?
            
            Selecione uma alternativa

            R: node.role == worker

            Alternativa correta! Informando o role e fazendo a comparação com ==, não teremos problemas.


Aula 05: Serviços globais e replicados -------

    Serviços replicados:

        Já vimos que nós managers também podem comportar serviço, por isso tivemos que usar regras de restrições para que o serviço ao ser criado não instânciasse tarefas nos nossos managers.

        Mas o ponto agora é o seguinte, quais serviços nós vamos querer rodar em nós do tipo manager? Vamos rodar serviços do tipo monitoramento, serviços de segurança, ou seja, serviços que
        são críticos, que dependemos na nossa aplicação.

        Porque queremos sempre estar monitorando o estado de qualquer nó do nosso Swarm, nós sempre queremos que eles estejam seguros, queremos sempre garantir que a nossa aplicação vai estar
        funcionando da maneira que esperamos, então como é que no Docker, aqui no Swarm, podemos fazer com que determinados serviços rodem em todos os nós da nossa aplicação? Até então estamos
        em cinco.

        Como é que eu posso garantir que, subindo de uma vez o serviço, ele rode em torno desses cinco nós? É uma pergunta a ser respondida, vamos responder isso daqui a pouco, mas antes de
        respondermos isso precisamos entender coo funciona os tipos de serviços do Docker Swarm.

        Nós tinhamos parado o serviço que estava rodando na ultima aula, vamos colocar ele para rodar novamente...

            $ docker service create -p 8080:3000 aluracursos/barbearia

        Vamos entender o que está acontecendo aqui, vamos começar a entender agora quais são os tipos de serviços que o Swarm tem para nós. Se dermos um "docker service ls" o que ele vai
        mostrar aqui?

            ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
            ts9vbn0i3cen        trusting_edison     replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp

        Temos o ID, o nome e agora sim o modo..."RELICATED"

        O modo aqui está como 'replicated' de replicado e tem uma réplica funcionando no total de uma réplica, o que isso significa? Significa que um dos modos do Docker aqui do Swarm replicado,
        e o outro é o que vamos ver na próxima aula, no próximo vídeo é o modo global, que é como podemos rodar um serviço em todos os nós.

        Mas o que é esse tal de modo replicado? Para começarmos a entender, esse modo replicado nada mais é que o tipo padrão de um serviço do Swarm e esse cara funciona do seguinte modo:
        Até então, o que estava acontecendo? Se dermos um "docker service ps "inicio id servico""...

            ID                  NAME                IMAGE                          NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
            vv8r8q9nwyre        trusting_edison.1   aluracursos/barbearia:latest   vm2                 Running             Running 5 minutes ago

        ...ele mostra que tem uma tarefa rodando na vm2 desse serviço que é o trusting_edison.

        Mas, vamos imaginar um caso real, vamos imaginar que aqui na nossa vm2 esses serviços estivessem rodando, tivessem recebendo diversas requisições, todas elas estariam sendo
        redirecionadas para vm2, por mais que a carga de receber pudesse ser balanceada por conta dos nossos outros nós através do Routing Mesh, tudo seria tratado no fim das contas pela vm2.

        A ideia é que possamos replicar alguns tipos de serviço para que eles sejam copiados, ter diversas cópias dele rodando em diversas máquinas, como assim? Ao invés de termos uma tarefa
        para esse serviço sendo executada, vamos ter na verdade quantas quisermos espalhadas pelo nosso Swarm para que todos os nós consigam dividir esse trabalho que vai ser executado.

        Até então temos uma tarefa, se eu quiser criar uma cópia de quatro tarefas para esse serviço, o que podemos fazer? Podemos simplesmente e utilizar o comando "docker service update",
        pois queremos mais uma ver atualizar o estado do nosso serviço, queremos atualizar o número de réplicas , então "--replicas", agora queremos definir um número de réplicas, por exemplo,
        quatro, e agora qual o serviço que queremos atualizar? Esse que acabamos de criar que é "ts9vbn0i3cen"...

            $ docker service update --replicas 4 ts9

            Saída:

                overall progress: 4 out of 4 tasks
                1/4: running   [==================================================>]
                2/4: running   [==================================================>]
                3/4: running   [==================================================>]
                4/4: running   [==================================================>]
                verify: Service converged

        Repare o que ele fez, alocou quatro tarefas para esse serviço, significa que agora se dermos um "docker service ps" com o id do serviço novamente...
        
            ID                  NAME                IMAGE                          NODE                DESIRED STATE       CURRENT STATE                ERROR               PORTS
            vv8r8q9nwyre        trusting_edison.1   aluracursos/barbearia:latest   vm2                 Running             Running 15 minutes ago
            40sbs4yh8njy        trusting_edison.2   aluracursos/barbearia:latest   vm4                 Running             Running about a minute ago
            0p2a0g0hgmgc        trusting_edison.3   aluracursos/barbearia:latest   vm3                 Running             Running about a minute ago
            k6cbg0axfsxg        trusting_edison.4   aluracursos/barbearia:latest   vm1                 Running             Running about a minute ago
        
        ...existem quatro tarefas que estão em execução para esse serviços espalhadas entre as nossas vms, 2, 4, 3, 1. Isso quer dizer que a partir desse momento, tem quatro réplicas desse
        mesmo serviço rodando espalhados pelo nosso Swarm.

        Então quando requisitarmos agora esse serviço, pode ser que ele caia tanto na vm1, quanto 2, 3 ou 4.

        Isso é bem legal porque agora efetivamente vai ter essa divisão de carga entre os nossos nós. Vamos dividir o serviço em mais réplicas para testarmos...

        Por exemplo, se atualizassemos de novo para 8 réplicas, o que o Swarm vai fazer? Ele vai alocar 8 cópias para essa tarefa desse serviço, e como eu só tenho cinco máquinas, alguma
        máquina vai ter mais de uma réplica não é verdade? 

            $ docker service update --replicas 8 ts9

            Saída:

                overall progress: 8 out of 8 tasks
                1/8: running   [==================================================>]
                2/8: running   [==================================================>]
                3/8: running   [==================================================>]
                4/8: running   [==================================================>]
                5/8: running   [==================================================>]
                6/8: running   [==================================================>]
                7/8: running   [==================================================>]
                8/8: running   [==================================================>]
                verify: Service converged

            $ docker service ps ts9

            Saída:

                ID                  NAME                IMAGE                          NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
                vv8r8q9nwyre        trusting_edison.1   aluracursos/barbearia:latest   vm2                 Running             Running 23 minutes ago
                40sbs4yh8njy        trusting_edison.2   aluracursos/barbearia:latest   vm4                 Running             Running 10 minutes ago
                0p2a0g0hgmgc        trusting_edison.3   aluracursos/barbearia:latest   vm3                 Running             Running 10 minutes ago
                k6cbg0axfsxg        trusting_edison.4   aluracursos/barbearia:latest   vm1                 Running             Running 10 minutes ago
                zlwle2js7h0u        trusting_edison.5   aluracursos/barbearia:latest   vm5                 Running             Running 2 minutes ago
                rkuc4eqhn8fn        trusting_edison.6   aluracursos/barbearia:latest   vm5                 Running             Running 2 minutes ago
                o4lg23vdkgz4        trusting_edison.7   aluracursos/barbearia:latest   vm1                 Running             Running 2 minutes ago
                mtnugys2q76o        trusting_edison.8   aluracursos/barbearia:latest   vm2                 Running             Running 2 minutes ago
        
        Podemos ver que temos as mesmas réplicas de antes, e mais uma na vm1, mais uma na vm2 e mais duas na vm5.

        Ou seja, ele criou diversas cópias desses serviços, diversas tarefas, que agora elas estão espalhadas pelo nosso Swarm, então se em algum momento algum desses caras não conseguir lidar
        com determinado número de requisições, outros também vão poder receber, porque esse serviços está espalhado pelo nosso Swarm.

        Isso é uma das possibilidades, esse é uma das possibilidades de distribuir um serviço pelo nosso Swarm, a outra possibilidade é, quando queremos um serviço rode em todos os nós do nosso
        Swarm, nós vamos querer trabalhar com serviço global, não vale a pena criarmos um serviço replicado com um alto número de réplicas só para garantir que ele vai estar em todos os nossos
        nós, isso já é meio que trapaça e vai diminuir nosso desempenho. A maneira correta para fazer isso é criando um serviço global.


    - Para saber mais: service scale

        Vamos supor que temos um serviço com id ci10k3u7q6ti. Como podemos escalar esse serviço para ter 5 réplicas?

        Aprendemos uma das possibilidades de alterar o número de réplicas de um serviço utilizando o comando docker service update --replicas 5 ci10k3u7q6ti, mas esse não é o único meio!
        
        Para isso também temos o comando docker service scale. Utilizando o id, podemos atualizar com o comando:
        
            $ docker service scale ci10k3u7q6ti=5

        Nesse caso, definimos 5 réplicas para o serviço. Os dois comandos produzem o mesmo resultado, o segundo é apenas uma forma resumida do primeiro comando.

    
    Serviços globais:

        Então vamos falar agora sobre os nossos serviços que vão ser criados em modo global, vimos ainda há pouco os serviços replicados, o que significa, como eles funcionam e tudo mais,
        mas agora vamos ver o nosso serviço global, que já falamos no início do vídeo passado que são os serviços que vão ter uma tarefa, uma instância desse serviço rodando em cada nó do
        nosso Swarm.

        E para que queremos isso? Queremos isso por algum motivo específico, por exemplo, garantir monitoramento, garantir segurança, queremos um serviço que seja crítico para ser executado
        em todos os nós, todos os nós no nosso caso vão depender da estância desse serviço, então queremos garantir que ele seja executado em todos esses nós.

        E como é que fazemos isso? Vemos que por padrão, se dermos o nosso comando "docker service create" e criar novamente o serviço o que acontece? Já sabemos que ele vai criar um réplica
        dele, vai criar ele no modo replicado e vai alocar ele aleatoriamente em uma máquina virtual, em algum nó do nosso Swarm.

        Mas e se quisermos criar esse serviço em modo global? Se quisermos que esse serviço tenha uma instância dele rodando em cada nó do nosso Swarm, como é que nós podemos garantir isso?
        Basta que, no meio do comando, antes de nós explicitarmos a imagem que queremos utilizar, podemos definir o modo em que esse serviço vai ser executado.

        Poderíamos colocar "--mode replicated", que é o padrão, ele já cria um serviço em modo replicado, ou podemos colocar "global", ou seja, global ele vai criar esse serviço em modo global
        e o que vai acontecer agora? Qual vai ser a grande diferença? A grande diferença é que de uma só vez ele vai começar a criar cinco instância desse serviço, cinco tarefas em diferentes
        nós, em todos os nós do Swarm.

            $ docker service create -p 8080:3000 --mode global aluracursos/barbearia

            Saída:

                overall progress: 5 out of 5 tasks
                in3t87hcu77q: running   [==================================================>]
                q5duspz3uggl: running   [==================================================>]
                rzmb7ke49zeo: running   [==================================================>]
                jh0vk0wrgiru: running   [==================================================>]
                mr4jjwio2une: running   [==================================================>]
                verify: Service converged

        Se dermos um "docker service ls", o que ele irá mostrar?

            ID                  NAME                  MODE                REPLICAS            IMAGE                          PORTS
            ck9nppwfl8wr        compassionate_raman   global              5/5                 aluracursos/barbearia:latest   *:8080->3000/tcp

        Ele está mostrando que tem cinco réplicas no total de cinco esperadas funcionando, mas agora o modo é global. Se utilizarmos agora o "docker service ps "id serviço""...

            ID                  NAME                                            IMAGE                          NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
            h2qr8wffwhzb        compassionate_raman.rzmb7ke49zeocehp5liaq97ez   aluracursos/barbearia:latest   vm5                 Running             Running 12 minutes ago
            97ka1hotzs1x        compassionate_raman.q5duspz3ugglfw664ln3xr18n   aluracursos/barbearia:latest   vm1                 Running             Running 12 minutes ago
            oon8gbxhb7y4        compassionate_raman.jh0vk0wrgiruzrejjhfor8l3u   aluracursos/barbearia:latest   vm4                 Running             Running 12 minutes ago
            xxb127sp3ten        compassionate_raman.mr4jjwio2une2dn51od448g9q   aluracursos/barbearia:latest   vm3                 Running             Running 12 minutes ago
            5qf60a5h3iob        compassionate_raman.in3t87hcu77qnk13ln538rg29   aluracursos/barbearia:latest   vm2                 Running             Running 12 minutes ago

        Ele mostra que em todos os nossos nós está rodando um tarefa, uma instância desse serviço.

        Isso significa que no dispatcher, no momento em que definimos qual nó vai carregar determinado serviço, nesse ponto vai ser diferente, o dispatcher vai dizer "como o serviço é global
        eu quero que todos os nós carreguem uma réplica desse serviço", ou seja, o serviço vai ser global e todos os nós vão ter uma instância dele.

        Então se caso for alguma aplicação crítica, ela vai estar sendo executada em todos os nossos nós, que talvez seja o que queiramos, ou não, depende, mas no fim das contas, usar só o
        serviço global ou só serviço replicado para tudo não é a solução, no mundo real, utilizaremos num mesmo Swarm serviços globais e serviços replicados.
        
    
    Nesta aula, aprendemos:

        - Serviços replicados rodam em um ou mais nós do swarm
        - Serviços globais rodam em todos os nós do swarm
        - Nós managers por padrão trabalham como workers
        - Serviços como monitoramento e segurança são bons exemplos de serviços globais
        - Como definir o modo que o serviço será criado utilizando a flag --mode no momento da criação do serviço

    
    Questões aula 05:

        1 - No último vídeo, aprendemos diversos aspectos sobre serviços replicados. Quais das alternativas abaixo são verdadeiras sobre esse tipo de serviço?

            Selecione 2 alternativas

            R1: Serviços replicados podem rodar em apenas um nó.

                Alternativa correta! Basta definirmos para o serviço ter apenas uma réplica.

            R2: Serviços por padrão são criados no modo replicado.

                Alternativa correta! Quando não informamos o modo desejado, criamos serviços replicados por padrão.

        
        2 - No último vídeo, aprendemos diversos aspectos sobre serviços globais. Quais das alternativas abaixo são verdadeiras sobre esse tipo de serviço?

            Selecione uma alternativa

            R: Bons exemplos de serviços globais são serviços de monitoramento e segurança.

            Alternativa correta! Serviços que são críticos à aplicação como um todo podem e devem ser executados como globais para que todos os nós possam ser devidamente monitorados e 
            estejam seguros.